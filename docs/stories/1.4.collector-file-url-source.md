# Story 1.4 â€“ Collector: File & URL Log Source

**Status:** Draft

## Story Statement
As an **SRE**, I want the MCP Log Collector to ingest logs from a **local file path** or **HTTP(S) URL**, so that I can summarise historical or remote log archives without relying on Docker container access.

## Acceptance Criteria
1. CLI flag `--file <path>` OR `--url <https://...>` is accepted (mutually exclusive with `--all` / `--label`).
2. File source:
   â€¢ Streams line-by-line via `fs.createReadStream` + `readline`, respecting existing 30 s / 8 KB chunk policy.  
   â€¢ When `--follow` is provided, tails the file and survives log rotation (similar to `tail -F`).
3. URL source:
   â€¢ Downloads and streams the resource (supports **gzip**) using `undici.stream` or `got.pipeline`.  
   â€¢ Supports HTTP range resume if connection drops before completion.
4. Redaction & ANSI-strip logic identical to Docker path; emitted metadata contains `source_id` set to `file:<abs_path>` or `url:<href>` (replaces `container_id`).
5. New Prometheus counter `collector_source_errors_total{source_type}` increments on I/O or network errors.
6. Integration tests:  
   â€¢ File: ingest `test/fixtures/sample.log`; assert chunks persisted to `chunks` table.  
   â€¢ URL: spin up local HTTP server serving gzipped log; assert same.  
   â€¢ Validate Prometheus metrics increment and summariser flow continues.
7. Circuit-breaker design unchanged; summariser consumes new chunks seamlessly.
8. Documentation: update `README.md`, CLI `--help`, OpenAPI spec if applicable.
9. Performance: maintain P95 end-to-end chunk latency < **250 ms** and CPU â‰¤ **5 %** at 2 000 lps from file stream.

## Tasks / Subtasks
- [ ] (AC 1) Extend CLI argument parser (`src/index.js`) to accept `--file`, `--url` (mutex group).  
  - [ ] Update help text and validation logic.
- [ ] (AC 2) Implement `FileChunker` subclass handling stream & rotation.  
  - [ ] Write unit tests for file read, follow, rotation.
- [ ] (AC 3) Implement `URLChunker` util wrapping HTTP stream & gzip decode.  
  - [ ] Retry & resume logic on failure.
- [ ] (AC 4) Refactor common redaction/metadata code for reuse across sources.
- [ ] (AC 5) Add Prometheus counter in `src/metrics.js`; increment in error paths.
- [ ] (AC 6) Create integration tests in `test/integration/collector-file-url.integration.test.js` (Vitest).  
  - [ ] Fixture server with `fastify` serving gzipped file.
- [ ] (AC 8) Update docs & OpenAPI; ensure CI lint passes.
- [ ] (AC 9) Benchmark file ingestion using `autocannon` and log generator; tune if needed.

## Dev Notes
* **Modules & APIs:**  
  - Use Node.js `fs` with `createReadStream` + `readline` for efficient line iteration.  
  - For HTTP, prefer `undici` streaming pipeline; fallback to `got` if needed.  
  - Decompression via `zlib.createGunzip()` piped into readline.  
* **Shared Interface:** Introduce `AbstractSourceReader` to unify Docker, File, URL implementations.
* **Metrics:** Label `source_type` as `docker`, `file`, or `url` for all existing counters/histograms.
* **Error Handling:** Bubble up I/O errors; circuit-breaker lives higher in summariser.
* **Logging:** Prefix emoji per style e.g. `ðŸ“‚ [collector/file]` or `ðŸŒ [collector/url]`.

### Testing Standards
* All tests under `test/` with `.test.js` suffix using Vitest + vi.mocks.  
* Achieve â‰¥ 90 % coverage on new code; no linter warnings.  
* Integration tests run in CI with in-process HTTP server; no external network.

## Change Log
| Date | Version | Description | Author |
|------|---------|-------------|--------| 